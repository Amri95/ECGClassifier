{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BayesClassifierECG.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "DfByttJIWbgQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Bayesian Classification for ECG Time-Series\n",
        "\n",
        "> Copyright 2019 Dave Fernandes. All Rights Reserved.\n",
        "> \n",
        "> Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "> you may not use this file except in compliance with the License.\n",
        "> You may obtain a copy of the License at\n",
        ">\n",
        "> http://www.apache.org/licenses/LICENSE-2.0\n",
        ">  \n",
        "> Unless required by applicable law or agreed to in writing, software\n",
        "> distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "> See the License for the specific language governing permissions and\n",
        "> limitations under the License."
      ]
    },
    {
      "metadata": {
        "id": "MTd4aHhYWhdN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "This notebook classifies time-series for segmented heartbeats from ECG lead II recordings. Either of two models (CNN or RNN) can be selected from training and evaluation.\n",
        "- Data for this analysis should be prepared using the `PreprocessECG.ipynb` notebook from this project.\n",
        "- Original data is from: https://www.kaggle.com/shayanfazeli/heartbeat"
      ]
    },
    {
      "metadata": {
        "id": "hjmdX-HbWdeI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as keras\n",
        "import tensorflow_probability as tfp\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "# tf.enable_eager_execution()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "TRAIN_SET = '/content/drive/My Drive/Colab Notebooks/Data/train_set.pickle'\n",
        "TEST_SET = '/content/drive/My Drive/Colab Notebooks/Data/test_set.pickle'\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "with open(TEST_SET, 'rb') as file:\n",
        "    test_set = pickle.load(file)\n",
        "    x_test = test_set['x']\n",
        "    y_test = test_set['y']\n",
        "\n",
        "with open(TRAIN_SET, 'rb') as file:\n",
        "    train_set = pickle.load(file)\n",
        "    x_train = train_set['x']\n",
        "    y_train = train_set['y']\n",
        "\n",
        "TRAIN_COUNT = len(y_train)\n",
        "TEST_COUNT = len(y_test)\n",
        "BATCHES_PER_EPOCH = TRAIN_COUNT // BATCH_SIZE\n",
        "INPUT_SIZE = np.shape(x_train)[1]\n",
        "\n",
        "print('Train size:', TRAIN_COUNT, 'Test size:', TEST_COUNT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x2Q8Cy8HZ8dB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Input Datasets"
      ]
    },
    {
      "metadata": {
        "id": "uiTIHzr5Wsmn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def combined_dataset(features, labels):\n",
        "    assert features.shape[0] == labels.shape[0]\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((np.expand_dims(features, axis=-1), labels))\n",
        "    return dataset\n",
        "\n",
        "# For training\n",
        "def train_input_fn():\n",
        "    dataset = combined_dataset(x_train, y_train)\n",
        "    return dataset.repeat().shuffle(TRAIN_COUNT).batch(BATCH_SIZE, drop_remainder=True).prefetch(1)\n",
        "\n",
        "# For evaluation and metrics\n",
        "def eval_input_fn():\n",
        "    dataset = combined_dataset(x_test, y_test)\n",
        "    return dataset.repeat().batch(TEST_COUNT).prefetch(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PnGsC48GaGTk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the model\n",
        "#### Convolutional Model\n",
        "* The convolutional model is taken from: https://arxiv.org/pdf/1805.00794.pdf\n",
        "\n",
        "Model consists of:\n",
        "* An initial 1-D convolutional layer\n",
        "* 5 repeated residual blocks (`same` padding)\n",
        "* A fully-connected layer\n",
        "* A linear layer with softmax output"
      ]
    },
    {
      "metadata": {
        "id": "roQZ8ZPqDWI9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MODEL_DIR = '/content/drive/My Drive/Colab Notebooks/Models/CNN'\n",
        "\n",
        "def conv_unit(unit, input_layer):\n",
        "    s = '_' + str(unit)\n",
        "    layer = keras.Convolution1D(name='Conv1' + s, filters=32, kernel_size=5, strides=1, padding='same', activation='relu')(input_layer)\n",
        "    layer = keras.Convolution1D(name='Conv2' + s, filters=32, kernel_size=5, strides=1, padding='same', activation=None)(layer )\n",
        "    layer = keras.Add(name='ResidualSum' + s)([layer, input_layer])\n",
        "    layer = keras.Activation(\"relu\", name='Act' + s)(layer)\n",
        "    layer = keras.MaxPooling1D(name='MaxPool' + s, pool_size=5, strides=2)(layer)\n",
        "    return layer\n",
        "\n",
        "def cnn_model(input_shape):\n",
        "    time_series = keras.Input(shape=input_shape, dtype='float32')\n",
        "    current_layer = keras.Convolution1D(filters=32, kernel_size=5, strides=1)(time_series)\n",
        "\n",
        "    for i in range(5):\n",
        "        current_layer = conv_unit(i + 1, current_layer)\n",
        "\n",
        "    current_layer = keras.Flatten()(current_layer)\n",
        "    current_layer = keras.Dense(32, name='FC1', activation='relu')(current_layer)\n",
        "    logits = keras.Dense(5, name='Output')(current_layer)\n",
        "    \n",
        "    model = tf.keras.Model(inputs=time_series, outputs=logits, name='cnn_model')\n",
        "    return model\n",
        "  \n",
        "# Standard cross-entropy loss\n",
        "categorical_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "def cnn_loss_fn(y_true, y_pred):\n",
        "    log_likelihood_loss = categorical_loss(y_true, y_pred)\n",
        "    return log_likelihood_loss\n",
        "\n",
        "def cnn_config():\n",
        "    return MODEL_DIR, cnn_model, cnn_loss_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5h3ticUaES-m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Bayesian CNN Model\n",
        "* Same network architecture as above\n",
        "* Flipout layers are used instead of standard layers"
      ]
    },
    {
      "metadata": {
        "id": "Q8XdxTVYaO1q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BAYES_MODEL_DIR = '/content/drive/My Drive/Colab Notebooks/Models/BayesianCNN'\n",
        "\n",
        "def bayes_conv_unit(unit, input_layer):\n",
        "    s = '_' + str(unit)\n",
        "    layer = tfp.layers.Convolution1DFlipout(name='Conv1' + s, filters=32, kernel_size=5, strides=1, padding='same', activation='relu')(input_layer)\n",
        "    layer = tfp.layers.Convolution1DFlipout(name='Conv2' + s, filters=32, kernel_size=5, strides=1, padding='same', activation=None)(layer )\n",
        "    layer = keras.Add(name='ResidualSum' + s)([layer, input_layer])\n",
        "    layer = keras.Activation(\"relu\", name='Act' + s)(layer)\n",
        "    layer = keras.MaxPooling1D(name='MaxPool' + s, pool_size=5, strides=2)(layer)\n",
        "    return layer\n",
        "\n",
        "def bayes_cnn_model(input_shape):\n",
        "    time_series = tf.keras.layers.Input(shape=input_shape, dtype='float32')\n",
        "#     current_layer = tfp.layers.Convolution1DFlipout(filters=32, kernel_size=5, strides=1)(time_series)\n",
        "    current_layer = keras.Convolution1D(filters=32, kernel_size=5, strides=1)(time_series)\n",
        "\n",
        "    for i in range(5):\n",
        "        current_layer = conv_unit(i + 1, current_layer)\n",
        "#         current_layer = bayes_conv_unit(i + 1, current_layer)\n",
        "\n",
        "    current_layer = keras.Flatten()(current_layer)\n",
        "#     current_layer = tfp.layers.DenseFlipout(32, name='FC1', activation='relu')(current_layer)\n",
        "    current_layer = keras.Dense(32, name='FC1', activation='relu')(current_layer)\n",
        "    logits = tfp.layers.DenseFlipout(5, name='Output')(current_layer)\n",
        "    \n",
        "    model = tf.keras.Model(inputs=time_series, outputs=logits, name='bayes_cnn_model')\n",
        "    return model\n",
        "  \n",
        "# Compute the negative Error Lower Bound loss\n",
        "bayes_categorical_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "def bayes_loss_fn(y_true, y_pred):\n",
        "    log_likelihood_loss = bayes_categorical_loss(y_true, y_pred)\n",
        "    \n",
        "    # Use Bayes by Backprop heuristic to weight KL-divergence\n",
        "#     global_step = tf.train.get_or_create_global_step()\n",
        "#     step = global_step % BATCHES_PER_EPOCH\n",
        "#     pi = tf.cond(step < 20, lambda: tf.pow(2.0, -tf.to_float(step)), lambda: tf.constant(0, dtype=tf.float32))\n",
        "\n",
        "    # Spread the KL-divergence over the full epoch\n",
        "    pi = 1 / BATCHES_PER_EPOCH\n",
        "    \n",
        "    kl = pi * sum(model.losses)\n",
        "    return log_likelihood_loss + kl\n",
        "\n",
        "def bayes_config():\n",
        "    return BAYES_MODEL_DIR, bayes_cnn_model, bayes_loss_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1h96cjFraUo_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train model"
      ]
    },
    {
      "metadata": {
        "id": "UvnL1RO4aVOK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow.distributions as tfd\n",
        "\n",
        "# Initial learning rate\n",
        "INITIAL_LEARNING_RATE = 0.001\n",
        "\n",
        "# Learning rate decay per LR_DECAY_STEPS steps (1.0 = no decay)\n",
        "LR_DECAY_RATE = 0.5\n",
        "\n",
        "# Number of steps for LR to decay by LR_DECAY_RATE\n",
        "LR_DECAY_STEPS = 4000\n",
        "\n",
        "model_dir, model_fn, loss_fn = bayes_config()\n",
        "\n",
        "model = model_fn([INPUT_SIZE, 1])\n",
        "\n",
        "learning_rate = INITIAL_LEARNING_RATE\n",
        "# global_step = tf.train.get_or_create_global_step()\n",
        "# learning_rate = tf.train.exponential_decay(INITIAL_LEARNING_RATE, global_step, LR_DECAY_STEPS, LR_DECAY_RATE)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "model.compile(loss=loss_fn, optimizer=optimizer, metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "# model.summary()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    model.fit(train_input_fn(), validation_data=eval_input_fn(), epochs=2, steps_per_epoch=BATCHES_PER_EPOCH, validation_steps=1)\n",
        "\n",
        "# strategy = tf.contrib.distribute.MirroredStrategy()\n",
        "# config = tf.estimator.RunConfig(train_distribute=strategy)\n",
        "# estimator = tf.keras.estimator.model_to_estimator(keras_model=model, config=config, model_dir=model_dir)\n",
        "# estimator.train(input_fn=train_input_fn, steps=BATCHES_PER_EPOCH * 4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nM4YXCgeaga6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Compute metrics"
      ]
    },
    {
      "metadata": {
        "id": "mhCK0g-faiqI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sklearn.metrics as skm\n",
        "\n",
        "x = np.expand_dims(x_test, axis=-1)\n",
        "y_logits = model.predict(x)\n",
        "y_predicted = np.argmax(y_logits, axis=1)\n",
        "y_test = np.reshape(y_test, (len(y_test), 1))\n",
        "\n",
        "# Classification report\n",
        "report = skm.classification_report(y_test, y_predicted)\n",
        "print(report)\n",
        "\n",
        "# Confusion matrix\n",
        "conf = skm.confusion_matrix(y_test, y_predicted)\n",
        "print(conf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BtlFRg6Daotn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "correct = []\n",
        "false_pos = []\n",
        "false_neg = []\n",
        "\n",
        "for k in range(5):\n",
        "    for i in range(len(y_test)):\n",
        "        if y_test[i] == k and y_predicted[i] == k:\n",
        "            correct.append(y_prob[i, k])\n",
        "        elif y_test[i] == k and y_predicted[i] != k:\n",
        "            false_neg.append(y_prob[i, k])\n",
        "        elif y_test[i] != k and y_predicted[i] == k:\n",
        "            false_pos.append(y_prob[i, k])\n",
        "\n",
        "n, bins, patches = plt.hist(correct, 20, (0, 1))\n",
        "plt.xlabel('Probability')\n",
        "plt.title('Correctly Classified')\n",
        "plt.show();\n",
        "\n",
        "n, bins, patches = plt.hist(false_pos, 20, (0, 1))\n",
        "plt.xlabel('Probability')\n",
        "plt.title('False Positives')\n",
        "plt.show();\n",
        "\n",
        "n, bins, patches = plt.hist(false_neg, 20, (0, 1))\n",
        "plt.xlabel('Probability')\n",
        "plt.title('False Negatives')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}