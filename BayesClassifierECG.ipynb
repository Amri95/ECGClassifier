{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BayesClassifierECG.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "DfByttJIWbgQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Bayesian Classification for ECG Time-Series\n",
        "\n",
        "> Copyright 2019 Dave Fernandes. All Rights Reserved.\n",
        "> \n",
        "> Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "> you may not use this file except in compliance with the License.\n",
        "> You may obtain a copy of the License at\n",
        ">\n",
        "> http://www.apache.org/licenses/LICENSE-2.0\n",
        ">  \n",
        "> Unless required by applicable law or agreed to in writing, software\n",
        "> distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "> See the License for the specific language governing permissions and\n",
        "> limitations under the License."
      ]
    },
    {
      "metadata": {
        "id": "MTd4aHhYWhdN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "This notebook classifies time-series for segmented heartbeats from ECG lead II recordings. Either of two models (CNN or RNN) can be selected from training and evaluation.\n",
        "- Data for this analysis should be prepared using the `PreprocessECG.ipynb` notebook from this project.\n",
        "- Original data is from: https://www.kaggle.com/shayanfazeli/heartbeat"
      ]
    },
    {
      "metadata": {
        "id": "hjmdX-HbWdeI",
        "colab_type": "code",
        "outputId": "c4d98316-f3b7-483e-d2ad-245b9636c90e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as keras\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "# tf.enable_eager_execution()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "TRAIN_SET = '/content/drive/My Drive/Colab Notebooks/Data/train_set.pickle'\n",
        "TEST_SET = '/content/drive/My Drive/Colab Notebooks/Data/test_set.pickle'\n",
        "\n",
        "with open(TEST_SET, 'rb') as file:\n",
        "    test_set = pickle.load(file)\n",
        "    x_test = test_set['x']\n",
        "    y_test = test_set['y']\n",
        "\n",
        "with open(TRAIN_SET, 'rb') as file:\n",
        "    train_set = pickle.load(file)\n",
        "    x_train = train_set['x']\n",
        "    y_train = train_set['y']\n",
        "    \n",
        "print('Train size:', len(y_train), 'Test size:', len(y_test))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Train size: 441841 Test size: 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x2Q8Cy8HZ8dB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Input Datasets"
      ]
    },
    {
      "metadata": {
        "id": "uiTIHzr5Wsmn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def combined_dataset(features, labels):\n",
        "    assert features.shape[0] == labels.shape[0]\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((np.expand_dims(features, axis=-1), labels))\n",
        "    return dataset\n",
        "\n",
        "# For training\n",
        "def train_input_fn():\n",
        "    dataset = combined_dataset(x_train, y_train)\n",
        "    return dataset.repeat().shuffle(500000).batch(256, drop_remainder=True).prefetch(1)\n",
        "\n",
        "# For evaluation and metrics\n",
        "def eval_input_fn():\n",
        "    dataset = combined_dataset(x_test, y_test)\n",
        "    return dataset.repeat().batch(500).prefetch(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PnGsC48GaGTk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the model\n",
        "#### Bayesian CNN Model\n",
        "* The convolutional model is taken from: https://arxiv.org/pdf/1805.00794.pdf\n",
        "\n",
        "Model consists of:\n",
        "* An initial 1-D convolutional layer\n",
        "* 5 repeated residual blocks (`same` padding)\n",
        "* A fully-connected layer\n",
        "* A linear layer with softmax output\n",
        "* Flipout layers are used instead of standard layers"
      ]
    },
    {
      "metadata": {
        "id": "roQZ8ZPqDWI9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MODEL_DIR = '/content/drive/My Drive/Colab Notebooks/Models/CNN'\n",
        "\n",
        "def conv_unit(unit, input_layer):\n",
        "    s = '_' + str(unit)\n",
        "    layer = keras.Convolution1D(name='Conv1' + s, filters=32, kernel_size=5, strides=1, padding='same', activation='relu')(input_layer)\n",
        "    layer = keras.Convolution1D(name='Conv2' + s, filters=32, kernel_size=5, strides=1, padding='same', activation=None)(layer )\n",
        "    layer = keras.Add(name='ResidualSum' + s)([layer, input_layer])\n",
        "    layer = keras.Activation(\"relu\", name='Act' + s)(layer)\n",
        "    layer = keras.MaxPooling1D(name='MaxPool' + s, pool_size=5, strides=2)(layer)\n",
        "    return layer\n",
        "\n",
        "def cnn_model(input_shape):\n",
        "    time_series = keras.Input(shape=input_shape, dtype='float32')\n",
        "    current_layer = keras.Convolution1D(filters=32, kernel_size=5, strides=1)(time_series)\n",
        "\n",
        "    for i in range(5):\n",
        "        current_layer = conv_unit(i + 1, current_layer)\n",
        "\n",
        "    current_layer = keras.Flatten()(current_layer)\n",
        "    current_layer = keras.Dense(32, name='FC1', activation='relu')(current_layer)\n",
        "    logits = keras.Dense(5, name='Output')(current_layer)\n",
        "    \n",
        "    model = tf.keras.Model(inputs=time_series, outputs=logits, name='cnn_model')\n",
        "    return model\n",
        "  \n",
        "# Standard cross-entropy loss\n",
        "categorical_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "def loss_fn(y_true, y_pred):\n",
        "    log_likelihood_loss = categorical_loss(y_true, y_pred)\n",
        "    return log_likelihood_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q8XdxTVYaO1q",
        "colab_type": "code",
        "outputId": "415ab19f-c79d-424c-f3b7-c7d7a51e2736",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow_probability as tfp\n",
        "\n",
        "BAYES_MODEL_DIR = '/content/drive/My Drive/Colab Notebooks/Models/BayesianCNN'\n",
        "\n",
        "def bayes_conv_unit(unit, input_layer):\n",
        "    s = '_' + str(unit)\n",
        "    layer = tfp.layers.Convolution1DFlipout(name='Conv1' + s, filters=32, kernel_size=5, strides=1, padding='same', activation='relu')(input_layer)\n",
        "    layer = tfp.layers.Convolution1DFlipout(name='Conv2' + s, filters=32, kernel_size=5, strides=1, padding='same', activation=None)(layer )\n",
        "    layer = keras.Add(name='ResidualSum' + s)([layer, input_layer])\n",
        "    layer = keras.Activation(\"relu\", name='Act' + s)(layer)\n",
        "    layer = keras.MaxPooling1D(name='MaxPool' + s, pool_size=5, strides=2)(layer)\n",
        "    return layer\n",
        "\n",
        "def bayes_cnn_model(input_shape):\n",
        "    time_series = tf.keras.layers.Input(shape=input_shape, dtype='float32')\n",
        "    current_layer = keras.Convolution1D(filters=32, kernel_size=5, strides=1)(time_series)\n",
        "\n",
        "    for i in range(5):\n",
        "        current_layer = conv_unit(i + 1, current_layer)\n",
        "\n",
        "    current_layer = keras.Flatten()(current_layer)\n",
        "    current_layer = keras.Dense(32, name='FC1', activation='relu')(current_layer)\n",
        "    logits = tfp.layers.DenseFlipout(5, name='Output')(current_layer)\n",
        "    \n",
        "    model = tf.keras.Model(inputs=time_series, outputs=logits, name='bayes_cnn_model')\n",
        "    return model\n",
        "  \n",
        "# Compute the -ELBO as the loss, averaged over the batch size.\n",
        "bayes_categorical_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "def bayes_loss_fn(y_true, y_pred):\n",
        "    log_likelihood_loss = bayes_categorical_loss(y_true, y_pred)\n",
        "    kl = sum(model.losses) / 1725\n",
        "    return log_likelihood_loss + kl"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1h96cjFraUo_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train model"
      ]
    },
    {
      "metadata": {
        "id": "UvnL1RO4aVOK",
        "colab_type": "code",
        "outputId": "66142d59-08b8-49e2-eff9-5816b5d275e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1005
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow.distributions as tfd\n",
        "\n",
        "# Initial learning rate\n",
        "INITIAL_LEARNING_RATE = 0.001\n",
        "\n",
        "# Learning rate decay per LR_DECAY_STEPS steps (1.0 = no decay)\n",
        "LR_DECAY_RATE = 0.5\n",
        "\n",
        "# Number of steps for LR to decay by LR_DECAY_RATE\n",
        "LR_DECAY_STEPS = 4000\n",
        "\n",
        "model = bayes_cnn_model([187, 1])\n",
        "\n",
        "# global_step = tf.train.get_global_step()\n",
        "# learning_rate = tf.train.exponential_decay(INITIAL_LEARNING_RATE, global_step, LR_DECAY_STEPS, LR_DECAY_RATE)\n",
        "learning_rate = INITIAL_LEARNING_RATE\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "model.compile(loss=bayes_loss_fn, optimizer=optimizer, metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "model.fit(train_input_fn(), validation_data=eval_input_fn(), epochs=2, steps_per_epoch=1725, validation_steps=1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/2\n",
            "1725/1725 [==============================] - 39s 23ms/step - loss: 276.0610 - sparse_categorical_accuracy: 0.8129 - val_loss: 156.1742 - val_sparse_categorical_accuracy: 0.8000\n",
            "Epoch 2/2\n",
            "1725/1725 [==============================] - 36s 21ms/step - loss: 73.2309 - sparse_categorical_accuracy: 0.4604 - val_loss: 17.2184 - val_sparse_categorical_accuracy: 0.1500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb0a50b3eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "nM4YXCgeaga6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Compute metrics"
      ]
    },
    {
      "metadata": {
        "id": "mhCK0g-faiqI",
        "colab_type": "code",
        "outputId": "92bf2aaa-04a3-41c5-dfbb-b68bae8500cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "import sklearn.metrics as skm\n",
        "\n",
        "x = np.expand_dims(x_test, axis=-1)\n",
        "y_logits = model.predict(x)\n",
        "y_predicted = np.argmax(y_logits, axis=1)\n",
        "y_test = np.reshape(y_test, (len(y_test), 1))\n",
        "\n",
        "# Classification report\n",
        "report = skm.classification_report(y_test, y_predicted)\n",
        "print(report)\n",
        "\n",
        "# Confusion matrix\n",
        "conf = skm.confusion_matrix(y_test, y_predicted)\n",
        "print(conf)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.17      0.01      0.02       100\n",
            "           1       0.00      0.00      0.00       100\n",
            "           2       0.25      0.02      0.04       100\n",
            "           3       0.06      0.03      0.04       100\n",
            "           4       0.17      0.72      0.27       100\n",
            "\n",
            "   micro avg       0.16      0.16      0.16       500\n",
            "   macro avg       0.13      0.16      0.07       500\n",
            "weighted avg       0.13      0.16      0.07       500\n",
            "\n",
            "[[ 1  0  1  3 95]\n",
            " [ 0  0  0  6 94]\n",
            " [ 4  0  2 22 72]\n",
            " [ 0  0  0  3 97]\n",
            " [ 1  2  5 20 72]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BtlFRg6Daotn",
        "colab_type": "code",
        "outputId": "f4b4328c-da32-4c0e-9f1b-ec5bd7deca9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "cell_type": "code",
      "source": [
        "correct = []\n",
        "false_pos = []\n",
        "false_neg = []\n",
        "\n",
        "for k in range(5):\n",
        "    for i in range(len(y_test)):\n",
        "        if y_test[i] == k and y_predicted[i] == k:\n",
        "            correct.append(y_prob[i, k])\n",
        "        elif y_test[i] == k and y_predicted[i] != k:\n",
        "            false_neg.append(y_prob[i, k])\n",
        "        elif y_test[i] != k and y_predicted[i] == k:\n",
        "            false_pos.append(y_prob[i, k])\n",
        "\n",
        "n, bins, patches = plt.hist(correct, 20, (0, 1))\n",
        "plt.xlabel('Probability')\n",
        "plt.title('Correctly Classified')\n",
        "plt.show();\n",
        "\n",
        "n, bins, patches = plt.hist(false_pos, 20, (0, 1))\n",
        "plt.xlabel('Probability')\n",
        "plt.title('False Positives')\n",
        "plt.show();\n",
        "\n",
        "n, bins, patches = plt.hist(false_neg, 20, (0, 1))\n",
        "plt.xlabel('Probability')\n",
        "plt.title('False Negatives')\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-f923d73b71e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my_predicted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mcorrect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my_predicted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mfalse_neg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_prob' is not defined"
          ]
        }
      ]
    }
  ]
}